{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final-2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAicqxJsIDwo",
        "outputId": "90bf0572-7f8f-42e1-e929-25ccdab9b75c"
      },
      "source": [
        "%cd 'drive/My Drive/Colab Notebooks/Project-Data'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Project-Data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsogIW7I2H8_"
      },
      "source": [
        "### Importing Libraries and training/testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0QRVeG7JBif"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import words, stopwords, wordnet\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "from scipy.sparse import csr_matrix, hstack\n",
        "from tqdm import tqdm\n",
        "import re, string"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTqHUNYecGca",
        "outputId": "b1843bfe-7bba-4c16-b046-c2ed5148786d"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I422_W-gJWjb",
        "outputId": "b91b7907-30d6-4ba5-d0f7-58120e86bac0"
      },
      "source": [
        "traindata = pd.read_csv('train.csv')\n",
        "testdata = pd.read_csv('test.csv')\n",
        "print('Training data Shape :', traindata.shape)\n",
        "print('Testing data Shape :', testdata.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data Shape : (783673, 3)\n",
            "Testing data Shape : (522449, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9Xe2C9v2Xak"
      },
      "source": [
        "### Importing Negative and Positivev words list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QPQ2_pjb1k9"
      },
      "source": [
        "negative_wordlist_path = '../negative-words.txt'\n",
        "positive_wordlist_path = '../positive-words.txt'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ8wYitsb3OB"
      },
      "source": [
        "def get_wordlist(path):\n",
        "  file = open(path)\n",
        "  words = []\n",
        "  \n",
        "  for w in file.readlines():\n",
        "    words.append(w.strip('\\n'))\n",
        "\n",
        "  return words"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7xF2z1jb49k"
      },
      "source": [
        "negative_words = get_wordlist(negative_wordlist_path)\n",
        "positive_words = get_wordlist(positive_wordlist_path)\n",
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaWqSTIm2sWm"
      },
      "source": [
        "### Creating Meta Features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfP9uoJwq3I4"
      },
      "source": [
        "traindata['punctuations'] = traindata['question_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
        "testdata['punctuations'] = testdata['question_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnk0lC-BPDuJ"
      },
      "source": [
        "def create_meta_features(data):\n",
        "    data['negative_words'] = data['question_text'].apply(lambda x: sum(x.count(w) for w in negative_words))\n",
        "    data['positive_words'] = data['question_text'].apply(lambda x: sum(x.count(w) for w in positive_words))\n",
        "    data['unique_words'] = data['question_text'].apply(lambda x: len(set(str(x).split())))\n",
        "    data['stop_words'] = data['question_text'].apply(lambda x: len([w for w in str(x).lower().split() if w in stop_words]))\n",
        "    data['uppercase_words'] = data['question_text'].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
        "    data['characters'] = data['question_text'].apply(lambda x: len(str(x)))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rp3eDJJZPDoK",
        "outputId": "ae50f68e-6503-4f66-b061-bf3afe19f4a1"
      },
      "source": [
        "print(\"Training data set shape before adding the meta features : \", traindata.shape)\n",
        "create_meta_features(traindata)\n",
        "print(\"Training data set shape after adding the meta features : \", traindata.shape)\n",
        "\n",
        "print(\"Testing data set shape before adding the meta features : \", testdata.shape)\n",
        "create_meta_features(testdata)\n",
        "print(\"Testing data set shape after adding the meta features : \", testdata.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data set shape before adding the meta features :  (783673, 3)\n",
            "Training data set shape after adding the meta features :  (783673, 9)\n",
            "Testing data set shape before adding the meta features :  (522449, 2)\n",
            "Testing data set shape after adding the meta features :  (522449, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDz2x0YDO9PN"
      },
      "source": [
        "traindata[['negative_words', 'positive_words', 'unique_words', 'stop_words', 'uppercase_words', 'characters']].to_csv('train_meta_features.csv', index=False)\n",
        "testdata[['negative_words', 'positive_words', 'unique_words', 'stop_words', 'uppercase_words', 'characters']].to_csv('test_meta_features.csv', index=False)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIxJ4IsFPJgY"
      },
      "source": [
        "train_meta = pd.read_csv('train_meta_features.csv')\n",
        "test_meta = pd.read_csv('test_meta_features.csv')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be4oohT822QZ"
      },
      "source": [
        "### Removing Contractions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FflXud3PKAs6"
      },
      "source": [
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \n",
        "                       \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \n",
        "                       \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \n",
        "                       \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \n",
        "                       \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \n",
        "                       \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n",
        "                       \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \n",
        "                       \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \n",
        "                       \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
        "                       \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n",
        "                       \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \n",
        "                       \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \n",
        "                       \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \n",
        "                       \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n",
        "                       \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \n",
        "                       \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \n",
        "                       \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \n",
        "                       \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n",
        "                       \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \n",
        "                       \"you're\": \"you are\", \"you've\": \"you have\" }"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aV39hOFKD7D"
      },
      "source": [
        "def clean_contractions(text, mapping):\n",
        "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
        "    for s in specials:\n",
        "        text = text.replace(s, \"'\")\n",
        "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
        "    return text"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpBneGL5KGdP"
      },
      "source": [
        "traindata['question_text'] = traindata['question_text'].apply(lambda x: clean_contractions(x, contraction_mapping))\n",
        "testdata['question_text'] = testdata['question_text'].apply(lambda x: clean_contractions(x, contraction_mapping))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wACa7hUaJ19Z"
      },
      "source": [
        "data = pd.concat(objs=[traindata['question_text'], testdata['question_text']], ignore_index=True)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcmgWfCO29xz"
      },
      "source": [
        "### Removing Punctuations and Performing Spellcheck."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tm7YLCZIclSn"
      },
      "source": [
        "to_replace = {\n",
        "    'banglore': 'bangalore',\n",
        "    'linsurance': 'insurance',\n",
        "    'neighbour': 'neighbor',\n",
        "    'favour': 'favor',\n",
        "    'tringle': 'triangle',\n",
        "    'favourite': 'favorite',\n",
        "    'labour': 'labor',\n",
        "    'newhouse': 'new house',\n",
        "    'bitcoins': 'bitcoin',\n",
        "    'centre': 'center',\n",
        "    'theatre': 'theater',\n",
        "    'quorans': 'quoran',\n",
        "    'quoran': 'quoran',\n",
        "    'origninal': 'original',\n",
        "    'jewellery': 'jewelery',\n",
        "    'gujaratis': 'gujarati',\n",
        "    'fiendly': 'friendly',\n",
        "    'organisation': 'organization',\n",
        "    'behaviour': 'behavior',\n",
        "    'iits': 'iit',\n",
        "    'iims': 'iim',\n",
        "    'iiits': 'iiit',\n",
        "    'cryptocurrencies': 'cryptocurrency',\n",
        "    'cancelled': 'canceled',\n",
        "    'bengaluru': 'bangalore',\n",
        "    'judgement': 'judgment',\n",
        "    'infty': 'nifty',\n",
        "    'fibre': 'fiber',\n",
        "    'specialisation': 'specialization',\n",
        "    'civilisation': 'civilization',\n",
        "    'upvoting': 'upvote',\n",
        "    'downvoting': 'downvote',\n",
        "    'upvotes': 'upvote',\n",
        "    'downvotes': 'downvote'\n",
        "}"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BVGbLwFJWhU",
        "outputId": "68004491-3c40-4189-ad78-f545489f2592"
      },
      "source": [
        "corpus = []\n",
        "\n",
        "for question in data:\n",
        "    words = re.sub('[^\\w\\s]', '', question)\n",
        "    corpus.append(words.lower())\n",
        "\n",
        "print(len(corpus))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1306122\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uISlRxJicom1",
        "outputId": "0a307cb3-aadd-41f9-d503-a1476802f00a"
      },
      "source": [
        "temp = []\n",
        "\n",
        "for sent in corpus:\n",
        "    res = []\n",
        "    for word in sent.split():\n",
        "        res.append(to_replace.get(word, word))\n",
        "    \n",
        "    temp.append(' '.join(res))\n",
        "\n",
        "corpus = temp\n",
        "\n",
        "print(len(corpus))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1306122\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3o4cSvO3OLN"
      },
      "source": [
        "### Using TfIdf to convert words to features for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Mfi0gRrOxmh"
      },
      "source": [
        "def get_sparse_matrix(meta_data, data, model):\n",
        "    vocab_features = model.transform(data)\n",
        "    meta_matrix = csr_matrix(meta_data)\n",
        "    vocab_features = hstack((vocab_features, meta_matrix), format='csr')\n",
        "\n",
        "    return vocab_features"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOizNAigJWem"
      },
      "source": [
        "tfidfv = TfidfVectorizer(ngram_range=(1, 4), max_features=40000, strip_accents='unicode', sublinear_tf=True, analyzer='char')\n",
        "tfidf_fit = tfidfv.fit(corpus)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fG7Ggzg7PO25"
      },
      "source": [
        "meta_features = ['negative_words', 'positive_words', 'stop_words', 'unique_words', 'uppercase_words', 'punctuations', 'characters']"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQlSLoelJWb1"
      },
      "source": [
        "X = get_sparse_matrix(train_meta[meta_features], corpus[0: traindata.shape[0]], tfidf_fit)\n",
        "y = traindata['target']\n",
        "testdata_features = get_sparse_matrix(test_meta[meta_features], corpus[traindata.shape[0]: len(corpus)], tfidf_fit)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-s-hCYYJWZR"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1024, stratify=traindata['target'])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_AJh-eXK41N"
      },
      "source": [
        "def bestThresshold(y_train,train_preds):\n",
        "    tmp = [0,0,0] # idx, cur, max\n",
        "    delta = 0\n",
        "    for tmp[0] in tqdm(np.arange(0.1, 0.501, 0.01)):\n",
        "        tmp[1] = f1_score(y_train, np.array(train_preds)>tmp[0])\n",
        "        if tmp[1] > tmp[2]:\n",
        "            delta = tmp[0]\n",
        "            tmp[2] = tmp[1]\n",
        "\n",
        "    print('\\nMax occurs at :', delta)\n",
        "    return tmp[2]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwY7U0apK43c"
      },
      "source": [
        "def model_train_cv(Xtrain, ytrain, Xtest, folds=5, maxiter=500, random_state=1024):\n",
        "    splits = list(StratifiedKFold(n_splits=folds, shuffle=True, random_state=1024).split(Xtrain, ytrain))\n",
        "\n",
        "    test_prob = np.zeros(Xtest.shape[0])\n",
        "    mul = 1 / folds\n",
        "    \n",
        "    for i in range(0, folds):\n",
        "        tr = splits[i][0]\n",
        "        te = splits[i][1]\n",
        "        x_train_fold = Xtrain[tr]\n",
        "        y_train_fold = ytrain.iloc[tr]\n",
        "        x_val_fold = Xtrain[te]\n",
        "        y_val_fold = ytrain.iloc[te]\n",
        "\n",
        "        clf = LogisticRegression(C=5, solver='liblinear', max_iter=maxiter)\n",
        "        clf.fit(x_train_fold, y_train_fold)\n",
        "        test_prob += (mul * clf.predict_proba(Xtest)[:, 1])\n",
        "    \n",
        "    return test_prob"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTP8P5yLJWWp",
        "outputId": "a97c6546-44df-4a95-85da-3524868b564f"
      },
      "source": [
        "test_predictions = model_train_cv(X_train, y_train, X_test, 5, 800)\n",
        "print (\"F1 Score: %0.3f \" % bestThresshold(y_test, test_predictions))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 41/41 [00:02<00:00, 15.46it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Max occurs at : 0.24999999999999992\n",
            "F1 Score: 0.626 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqr7LlUD3b2Z"
      },
      "source": [
        "### Making Test Submission file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8qSpVrnM3ix",
        "outputId": "4ef65954-9506-4ad0-c8e3-60bb696185f3"
      },
      "source": [
        "test_predictions = model_train_cv(X, y, testdata_features, 5, 800)\n",
        "\n",
        "pred = (test_predictions > 0.25).astype(np.int8)\n",
        "submissiondf = pd.DataFrame(data=testdata['qid'])\n",
        "submissiondf['target'] = pred\n",
        "submissiondf.to_csv('Submission.csv', index=False)\n",
        "\n",
        "print('Submisssion File Shape : ', submissiondf.shape)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Submisssion File Shape :  (522449, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}