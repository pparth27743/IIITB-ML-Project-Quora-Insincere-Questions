{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3XB9RVrUPVmM",
        "s8q1SW5lITdB",
        "GSCnV7JwI1W7",
        "dZh_obQtJDoi",
        "qQtlonlR18FU",
        "YpImSzZzaxOh",
        "cr9ql4NlcubP",
        "6p9B8tfWeB0h",
        "W5oXoAHEvzQG",
        "dY143YVEurNx"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YL5qyZDZHBve",
        "outputId": "63a7016e-8f94-4cb5-d196-40bcba34d14a"
      },
      "source": [
        "%cd 'drive/My Drive/Colab Notebooks/Project-Data'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Project-Data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XB9RVrUPVmM"
      },
      "source": [
        "\r\n",
        "# Setting up Libraries and Functions for PreProcessing\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8q1SW5lITdB"
      },
      "source": [
        "### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bQe-tILISma"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import nltk\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from matplotlib.gridspec import GridSpec\r\n",
        "import seaborn as sns\r\n",
        "from nltk.corpus import words, stopwords, wordnet\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier, LogisticRegression, LogisticRegressionCV\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "from gensim.models import KeyedVectors\r\n",
        "from scipy.sparse import csr_matrix, hstack\r\n",
        "import re, string\r\n",
        "from tqdm import tqdm\r\n",
        "import copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOlieHWFIZSq",
        "outputId": "48f1f256-e6b9-42d0-8001-f853227a3248"
      },
      "source": [
        "nltk.download('stopwords')\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSCnV7JwI1W7"
      },
      "source": [
        "### Reading Training and Testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kSCXHO1Iyyq"
      },
      "source": [
        "traindata = pd.read_csv('train.csv')\r\n",
        "testdata = pd.read_csv('test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZh_obQtJDoi"
      },
      "source": [
        "### Reading Positive and Negative Word List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCTUvLB3OpQT"
      },
      "source": [
        "negative_wordlist_path = '../negative-words.txt'\r\n",
        "positive_wordlist_path = '../positive-words.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tgaz0MN0Ot8a"
      },
      "source": [
        "def get_wordlist(path):\r\n",
        "  file = open(path)\r\n",
        "  words = []\r\n",
        "  \r\n",
        "  for w in file.readlines():\r\n",
        "    words.append(w.strip('\\n'))\r\n",
        "\r\n",
        "  return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2niX-E6lOxKq"
      },
      "source": [
        "negative_words = get_wordlist(negative_wordlist_path)\r\n",
        "positive_words = get_wordlist(positive_wordlist_path)\r\n",
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exVxsZQ5Phi6"
      },
      "source": [
        "# PreProcessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7K4Vmeru_cL"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\r\n",
        "def refine_data(data, removestopw):\r\n",
        "    corpus = []\r\n",
        "    temp = []\r\n",
        "\r\n",
        "    for question in data['question_text']:\r\n",
        "        words = re.sub('[^\\w\\s]', ' ', question)\r\n",
        "        words = words.lower()\r\n",
        "        words = words.split()\r\n",
        "        if removestopw == True:\r\n",
        "            words = [lemmatizer.lemmatize(word) for word in words if not word in stop_words]\r\n",
        "        else:\r\n",
        "            temp = []\r\n",
        "            for word in words:\r\n",
        "                if (word not in stop_words):\r\n",
        "                    temp.append(lemmatizer.lemmatize(word))\r\n",
        "                else:\r\n",
        "                    temp.append(word)\r\n",
        "            words = temp\r\n",
        "        \r\n",
        "        words = ' '.join(words)\r\n",
        "        corpus.append(words)\r\n",
        "\r\n",
        "    return corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjSk_I75u882"
      },
      "source": [
        "def bestThresshold(y_train,train_preds):\r\n",
        "    tmp = [0,0,0] # idx, cur, max\r\n",
        "    delta = 0\r\n",
        "    for tmp[0] in tqdm(np.arange(0.1, 0.501, 0.01)):\r\n",
        "        tmp[1] = f1_score(y_train, np.array(train_preds)>tmp[0])\r\n",
        "        if tmp[1] > tmp[2]:\r\n",
        "            delta = tmp[0]\r\n",
        "            tmp[2] = tmp[1]\r\n",
        "\r\n",
        "    print('\\nMax occurs at :', delta)\r\n",
        "    return tmp[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-DEUSsIU0eC"
      },
      "source": [
        "### Removing Contractions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PkEQ91PerKN"
      },
      "source": [
        "traindata['punctuations'] = traindata['question_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\r\n",
        "testdata['punctuations'] = testdata['question_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6l7fN_efFX3"
      },
      "source": [
        "Before removing the contractions, for the meta features, calculating the number of punctuations present in the corpus so that the information regarding number of puntuation marks in each question is not lost."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bA0aPCTi6LR"
      },
      "source": [
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \r\n",
        "                       \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \r\n",
        "                       \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \r\n",
        "                       \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \r\n",
        "                       \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \r\n",
        "                       \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\r\n",
        "                       \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \r\n",
        "                       \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \r\n",
        "                       \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \r\n",
        "                       \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \r\n",
        "                       \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \r\n",
        "                       \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \r\n",
        "                       \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \r\n",
        "                       \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \r\n",
        "                       \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \r\n",
        "                       \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \r\n",
        "                       \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \r\n",
        "                       \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\r\n",
        "                       \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \r\n",
        "                       \"you're\": \"you are\", \"you've\": \"you have\" }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hmaz2TqWycz"
      },
      "source": [
        "def clean_contractions(text, mapping):\r\n",
        "    specials = [\"’\", \"‘\", \"´\", \"`\"]\r\n",
        "    for s in specials:\r\n",
        "        text = text.replace(s, \"'\")\r\n",
        "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\r\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lcaw7EfhXC6B"
      },
      "source": [
        "traindata['question_text'] = traindata['question_text'].apply(lambda x: clean_contractions(x, contraction_mapping))\r\n",
        "testdata['question_text'] = testdata['question_text'].apply(lambda x: clean_contractions(x, contraction_mapping))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kXeQl3SO87b"
      },
      "source": [
        "### Creating Meta Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZSsYjWcO7lB"
      },
      "source": [
        "def create_meta_features(data):\r\n",
        "    data['negative_words'] = data['question_text'].apply(lambda x: sum(x.count(w) for w in negative_words))\r\n",
        "    data['positive_words'] = data['question_text'].apply(lambda x: sum(x.count(w) for w in positive_words))\r\n",
        "    data['unique_words'] = data['question_text'].apply(lambda x: len(set(str(x).split())))\r\n",
        "    data['stop_words'] = data['question_text'].apply(lambda x: len([w for w in str(x).lower().split() if w in stop_words]))\r\n",
        "    data['uppercase_words'] = data['question_text'].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\r\n",
        "    data['characters'] = data['question_text'].apply(lambda x: len(str(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_hSrZskPQsy",
        "outputId": "21d6e25f-4491-487c-bc83-47efdad6d7e6"
      },
      "source": [
        "print(\"Training data set shape before adding the meta features : \", traindata.shape)\r\n",
        "create_meta_features(traindata)\r\n",
        "print(\"Training data set shape after adding the meta features : \", traindata.shape)\r\n",
        "\r\n",
        "print(\"Testing data set shape before adding the meta features : \", testdata.shape)\r\n",
        "create_meta_features(testdata)\r\n",
        "print(\"Testing data set shape after adding the meta features : \", testdata.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data set shape before adding the meta features :  (783673, 3)\n",
            "Training data set shape after adding the meta features :  (783673, 10)\n",
            "Testing data set shape before adding the meta features :  (522449, 2)\n",
            "Testing data set shape after adding the meta features :  (522449, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCwh5CWMwPJv"
      },
      "source": [
        "traindata[['negative_words', 'positive_words', 'unique_words', 'stop_words', 'uppercase_words', 'punctuations', 'characters']].to_csv('train_meta_features.csv', index=False)\r\n",
        "testdata[['negative_words', 'positive_words', 'unique_words', 'stop_words', 'uppercase_words', 'punctuations', 'characters']].to_csv('test_meta_features.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S54WXm7EOqHk"
      },
      "source": [
        "train_meta = pd.read_csv('train_meta_features.csv')\r\n",
        "test_meta = pd.read_csv('test_meta_features.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQtlonlR18FU"
      },
      "source": [
        "### Replacing few words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nIRAqVO17Z5"
      },
      "source": [
        "to_replace = {\r\n",
        "    'banglore': 'bangalore',\r\n",
        "    'linsurance': 'insurance',\r\n",
        "    'neighbour': 'neighbor',\r\n",
        "    'favour': 'favor',\r\n",
        "    'tringle': 'triangle',\r\n",
        "    'favourite': 'favorite',\r\n",
        "    'labour': 'labor',\r\n",
        "    'newhouse': 'new house',\r\n",
        "    'bitcoins': 'bitcoin',\r\n",
        "    'centre': 'center',\r\n",
        "    'theatre': 'theater',\r\n",
        "    'quorans': 'quoran',\r\n",
        "    'quoran': 'quoran',\r\n",
        "    'origninal': 'original',\r\n",
        "    'jewellery': 'jewelery',\r\n",
        "    'gujaratis': 'gujarati',\r\n",
        "    'fiendly': 'friendly',\r\n",
        "    'organisation': 'organization',\r\n",
        "    'behaviour': 'behavior',\r\n",
        "    'iits': 'iit',\r\n",
        "    'iims': 'iim',\r\n",
        "    'iiits': 'iiit',\r\n",
        "    'cryptocurrencies': 'cryptocurrency',\r\n",
        "    'cancelled': 'canceled',\r\n",
        "    'bengaluru': 'bangalore',\r\n",
        "    'judgement': 'judgment',\r\n",
        "    'infty': 'nifty',\r\n",
        "    'fibre': 'fiber',\r\n",
        "    'specialisation': 'specialization',\r\n",
        "    'civilisation': 'civilization',\r\n",
        "    'upvoting': 'upvote',\r\n",
        "    'downvoting': 'downvote',\r\n",
        "    'upvotes': 'upvote',\r\n",
        "    'downvotes': 'downvote'\r\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EkH89VN2EHJ"
      },
      "source": [
        "def replacewords(corpus):\r\n",
        "    temp = []\r\n",
        "\r\n",
        "    for sent in corpus:\r\n",
        "        res = []\r\n",
        "        for word in sent.split():\r\n",
        "            res.append(to_replace.get(word, word))\r\n",
        "    \r\n",
        "        temp.append(' '.join(res))\r\n",
        "\r\n",
        "    return temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DYAKF2J2Wvq"
      },
      "source": [
        "traindata['question_text'] = replacewords(traindata['question_text'])\r\n",
        "testdata['question_text'] = replacewords(testdata['question_text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpImSzZzaxOh"
      },
      "source": [
        "### Lemmatization and Removal of Stop Words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwV-nNhoU3XT"
      },
      "source": [
        "traindata['question_text'] = refine_data(traindata, True)\r\n",
        "testdata['question_text'] = refine_data(testdata, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwqWYYhKc5pd"
      },
      "source": [
        "traindata.to_csv('train_temp.csv', index=False)\r\n",
        "testdata.to_csv('test_temp.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr9ql4NlcubP"
      },
      "source": [
        "### After removing Stopwords Looking for Null data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_npn-4lc0N9"
      },
      "source": [
        "traindata = pd.read_csv('train_temp.csv')\r\n",
        "testdata = pd.read_csv('test_temp.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqKZGor0dYUI",
        "outputId": "34738ea3-4485-418b-bd76-582b08055483"
      },
      "source": [
        "traindata.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "qid               0\n",
              "question_text    56\n",
              "target            0\n",
              "punctuations      0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Agz2MYIFjbYK",
        "outputId": "19e58dae-2945-42c1-e5e7-fe09af0e7639"
      },
      "source": [
        "testdata.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "qid               0\n",
              "question_text    32\n",
              "punctuations      0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-XFtowSjeO2"
      },
      "source": [
        "traindata['question_text'].fillna('question number stop word punctuation', inplace=True)\r\n",
        "testdata['question_text'].fillna('question number stop word punctuation', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJweUTvJjlAS"
      },
      "source": [
        "traindata.to_csv('train_clean.csv', index=False)\r\n",
        "testdata.to_csv('test_clean.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMMpS32cNTO9"
      },
      "source": [
        "# Converting to Vectors and training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p9B8tfWeB0h"
      },
      "source": [
        "## Using Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1qn-IunFJJM",
        "outputId": "57b73ab0-3a28-4907-95aa-dc0bec3802ea"
      },
      "source": [
        "traindata = pd.read_csv('train_clean.csv')\r\n",
        "testdata = pd.read_csv('test_clean.csv')\r\n",
        "print('Training data Shape :', traindata.shape)\r\n",
        "print('Testing data Shape :', testdata.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data Shape : (783673, 4)\n",
            "Testing data Shape : (522449, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGAwz0tXeBOt"
      },
      "source": [
        "word2vec_path = '../GoogleNews-vectors-negative300.bin'\r\n",
        "word2vec_model = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkLD0GhyeS4q"
      },
      "source": [
        "def get_embeddings(data):\r\n",
        "    sentences = []\r\n",
        "\r\n",
        "    for sent in data['question_text']:\r\n",
        "        sentences.append(nltk.word_tokenize(sent))\r\n",
        "\r\n",
        "    embeddings = np.zeros(shape=(len(sentences), 300))\r\n",
        "    unseen_words = []\r\n",
        "\r\n",
        "    for s_index, sent in enumerate(sentences):\r\n",
        "        temp = np.zeros(shape=(len(sent), 300))\r\n",
        "        count = 0\r\n",
        "        for w_index, word in enumerate(sent):\r\n",
        "            if word in word2vec_model.vocab:\r\n",
        "                temp[w_index] = word2vec_model.get_vector(word)\r\n",
        "            else:\r\n",
        "                count = count + 1\r\n",
        "\r\n",
        "        unseen_words.append(count)\r\n",
        "        embeddings[s_index] = temp.mean(axis=0)\r\n",
        "\r\n",
        "    return (embeddings, unseen_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ph9DqObugcKh"
      },
      "source": [
        "embeddings, unseen_words = get_embeddings(traindata)\r\n",
        "\r\n",
        "embeddings_df = pd.DataFrame(data=embeddings, columns=['d_' + str(i) for i in range(0, 300)])\r\n",
        "traindata = pd.concat([traindata, embeddings_df], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT8m-2BVgizN"
      },
      "source": [
        "#embeddings, unseen_words = get_embeddings(testdata)\r\n",
        "\r\n",
        "#embeddings_df = pd.DataFrame(data=embeddings, columns=['d_' + str(i) for i in range(0, 300)])\r\n",
        "#testdata = pd.concat([testdata, embeddings_df], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iQU44BOyceE"
      },
      "source": [
        "del word2vec_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cEQ-8D7g1TT"
      },
      "source": [
        "X = traindata.drop(columns=['question_text', 'qid', 'target'])\r\n",
        "y = traindata['target']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aOOClOTw6lz"
      },
      "source": [
        "X = pd.concat([X, train_meta], axis=1)\r\n",
        "y = traindata['target']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c34HkLq_xwsg"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0, stratify=y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQxAIEpMyP5m",
        "outputId": "452233ac-7486-45bd-c5d3-b31081fc004e"
      },
      "source": [
        "logistic_classifier = LogisticRegression(max_iter=500, C=5)\r\n",
        "logistic_classifier.fit(X_train, y_train)\r\n",
        "pred_prob = logistic_classifier.predict_proba(X_test)[:, 1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qT-LCYZyVLO",
        "outputId": "6668fe7e-ab82-414a-fbd9-93753e08788c"
      },
      "source": [
        "print (\"F1 Score: %0.3f \" % bestThresshold(y_test, pred_prob))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 41/41 [00:02<00:00, 15.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Max occurs at : 0.18999999999999995\n",
            "F1 Score: 0.523 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G38qGkbOKa1P"
      },
      "source": [
        "## Using TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRVzu74bKdRV",
        "outputId": "5d055874-d1e4-4c63-8696-61025968a229"
      },
      "source": [
        "traindata = pd.read_csv('train_clean.csv')\r\n",
        "testdata = pd.read_csv('test_clean.csv')\r\n",
        "print('Training data Shape :', traindata.shape)\r\n",
        "print('Testing data Shape :', testdata.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data Shape : (783673, 4)\n",
            "Testing data Shape : (522449, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kb45Gf8wKiLV"
      },
      "source": [
        "corpus = pd.concat(objs=[traindata['question_text'], testdata['question_text']], ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49rokQGEbv-p"
      },
      "source": [
        "tfidfv = TfidfVectorizer(ngram_range=(1, 2), max_features=55000)\r\n",
        "tfidf_fit = tfidfv.fit(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DB-TBa-kCBWc"
      },
      "source": [
        "meta_features = ['negative_words', 'positive_words', 'unique_words', 'stop_words', 'uppercase_words', 'punctuations', 'characters']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXrA-EXNLpG9"
      },
      "source": [
        "def get_sparse_matrix(meta_data, data, model):\r\n",
        "    vocab_features = model.transform(data)\r\n",
        "    meta_matrix = csr_matrix(meta_data)\r\n",
        "    vocab_features = hstack((vocab_features, meta_matrix), format='csr')\r\n",
        "\r\n",
        "    return vocab_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GYHLuBEM3b1"
      },
      "source": [
        "X = get_sparse_matrix(train_meta[meta_features], corpus[0: traindata.shape[0]], tfidf_fit)\r\n",
        "y = traindata['target']\r\n",
        "testdata_features = get_sparse_matrix(test_meta[meta_features], corpus[traindata.shape[0]: len(corpus)], tfidf_fit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkSu5_9GNDYb"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1024, stratify=traindata['target'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ6iqJtFNJu8"
      },
      "source": [
        "def model_train_cv(Xtrain, ytrain, Xtest, folds=5, maxiter=500, random_state=1024):\r\n",
        "    splits = list(StratifiedKFold(n_splits=folds, shuffle=True, random_state=1024).split(Xtrain, ytrain))\r\n",
        "\r\n",
        "    test_prob = np.zeros(Xtest.shape[0])\r\n",
        "    mul = 1 / folds\r\n",
        "    \r\n",
        "    for i in range(0, folds):\r\n",
        "        tr = splits[i][0]\r\n",
        "        te = splits[i][1]\r\n",
        "        x_train_fold = Xtrain[tr]\r\n",
        "        y_train_fold = ytrain.iloc[tr]\r\n",
        "        x_val_fold = Xtrain[te]\r\n",
        "        y_val_fold = ytrain.iloc[te]\r\n",
        "\r\n",
        "        clf = LogisticRegression(C=4, solver='liblinear', max_iter=maxiter)\r\n",
        "        clf.fit(x_train_fold, y_train_fold)\r\n",
        "        test_prob += (mul * clf.predict_proba(Xtest)[:, 1])\r\n",
        "    \r\n",
        "    return test_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJNSFougNLFa",
        "outputId": "9554cc02-40d5-4680-fe95-fdfb55222034"
      },
      "source": [
        "test_predictions = model_train_cv(X_train, y_train, X_test, 5, 800)\r\n",
        "print (\"F1 Score: %0.3f \" % bestThresshold(y_test, test_predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 41/41 [00:02<00:00, 15.37it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Max occurs at : 0.24999999999999992\n",
            "F1 Score: 0.601 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqG9dC4Kd2tn",
        "outputId": "ee286d8f-2ce8-4c46-e503-913c753bf841"
      },
      "source": [
        "multinomialnb_classifier = MultinomialNB()\r\n",
        "multinomialnb_classifier.fit(X_train, y_train)\r\n",
        "pred = multinomialnb_classifier.predict(X_test)\r\n",
        "print (\"F1 Score: %0.3f \" % bestThresshold(y_test, pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 41/41 [00:02<00:00, 15.82it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Max occurs at : 0.1\n",
            "F1 Score: 0.444 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q16pUPeNh0p-"
      },
      "source": [
        "# Best Submission Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bctDjbMrjDIe"
      },
      "source": [
        "traindata = pd.read_csv('train.csv')\r\n",
        "testdata = pd.read_csv('test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpEgTPB5i-wc"
      },
      "source": [
        "traindata['question_text'] = traindata['question_text'].apply(lambda x: clean_contractions(x, contraction_mapping))\r\n",
        "testdata['question_text'] = testdata['question_text'].apply(lambda x: clean_contractions(x, contraction_mapping))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceuzlsjwjKt0"
      },
      "source": [
        "data = pd.concat(objs=[traindata['question_text'], testdata['question_text']], ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_JSRKtsl5ok",
        "outputId": "bff18e38-d6aa-4e5a-dcc3-cd9dba3ef87e"
      },
      "source": [
        "corpus = []\r\n",
        "\r\n",
        "for question in data:\r\n",
        "    words = re.sub('[^\\w\\s]', '', question)\r\n",
        "    corpus.append(words.lower())\r\n",
        "\r\n",
        "print(len(corpus))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1306122\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWaJDFsjl8ih"
      },
      "source": [
        "tfidfv = TfidfVectorizer(ngram_range=(1, 4), max_features=40000, strip_accents='unicode', sublinear_tf=True, analyzer='char')\r\n",
        "tfidf_fit = tfidfv.fit(corpus)\r\n",
        "\r\n",
        "#X = tfidf_fit.transform(corpus[0: traindata.shape[0]])\r\n",
        "#y = traindata['target']\r\n",
        "#testdata_features = tfidf_fit.transform(corpus[traindata.shape[0]: ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnT_QO27CbHn"
      },
      "source": [
        "meta_features = ['negative_words', 'positive_words', 'unique_words', 'uppercase_words', 'punctuations']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWrzMDkFwgeO"
      },
      "source": [
        "X = get_sparse_matrix(train_meta[meta_features], corpus[0: traindata.shape[0]], tfidf_fit)\r\n",
        "y = traindata['target']\r\n",
        "testdata_features = get_sparse_matrix(test_meta[meta_features], corpus[traindata.shape[0]: len(corpus)], tfidf_fit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DLVQ_bArVBa"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1024, stratify=traindata['target'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVCv2qVbmlYj",
        "outputId": "bbaee1f7-2e13-48c2-b882-b3a8187eecab"
      },
      "source": [
        "test_predictions = model_train_cv(X_train, y_train, X_test, 5, 800)\r\n",
        "print (\"F1 Score: %0.3f \" % bestThresshold(y_test, test_predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 41/41 [00:02<00:00, 15.55it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Max occurs at : 0.2699999999999999\n",
            "F1 Score: 0.625 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhfIxghZCsRY"
      },
      "source": [
        "### Submission file Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HHiUgbUsz5X",
        "outputId": "f1d53a1e-7622-4bee-d660-c3e5217b7c71"
      },
      "source": [
        "test_predictions = model_train_cv(X, y, testdata_features, 5, 800)\r\n",
        "\r\n",
        "pred = (test_predictions > 0.25).astype(np.int8)\r\n",
        "submissiondf = pd.DataFrame(data=testdata['qid'])\r\n",
        "submissiondf['target'] = pred\r\n",
        "submissiondf.to_csv('Submission.csv', index=False)\r\n",
        "\r\n",
        "print('Submisssion File Shape : ', submissiondf.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Submisssion File Shape :  (522449, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}